{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# gpt3 notes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m nurse_gpt3_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./excel_sheets/nurse_labelled_notes_gpt3.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m nurse_gpt3_df \u001b[38;5;241m=\u001b[39m nurse_gpt3_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreport\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnurse_reports\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneeds\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnurse_labels\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m      4\u001b[0m gpt3_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./excel_sheets/gpt3_labelled_notes.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# gpt3 notes\n",
    "nurse_gpt3_df = pd.read_excel('./excel_sheets/nurse_labelled_notes_gpt3.xlsx')\n",
    "nurse_gpt3_df = nurse_gpt3_df.rename(columns={'report': 'nurse_reports', 'needs' : 'nurse_labels'})\n",
    "gpt3_df = pd.read_excel('./excel_sheets/gpt3_labelled_notes.xlsx')\n",
    "gpt3_df = gpt3_df.rename(columns={'report': 'gpt_reports', 'needs' : 'gpt_labels'})\n",
    "\n",
    "# gpt4 notes\n",
    "nurse_gpt4_df = pd.read_excel('./excel_sheets/nurse_labelled_notes_gpt4.xlsx')\n",
    "nurse_gpt4_df = nurse_gpt4_df.rename(columns={'report': 'nurse_reports', 'needs' : 'nurse_labels'})\n",
    "gpt4_df = pd.read_excel('./excel_sheets/gpt4_labelled_notes.xlsx')\n",
    "gpt4_df = gpt4_df.rename(columns={'report': 'gpt_reports', 'needs' : 'gpt_labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def notes_cleaning(nurse_df, gpt_df, remove_comment_notes = False):\n",
    "    # put all labels together\n",
    "    compare_df = pd.concat([gpt_df, nurse_df], axis=1)\n",
    "    # step 1 - find number of 'problematic notes' i.e., notes that were not deemed to be classifiable \n",
    "    mask = compare_df[['comments']].isnull().any(axis=1)\n",
    "    comment_df = compare_df.dropna(subset=['comments'])\n",
    "    print(f\"\\n---------\\nPercentage of problematic notes ~{round((len(comment_df) / len(compare_df)) * 100, 2)}%\")\n",
    "\n",
    "    # step 2 - clean full dataframe to remove any unwanted information\n",
    "    if remove_comment_notes:\n",
    "        compare_df = compare_df[mask]\n",
    "    cleaned_df = compare_df.drop(columns=['nurse_reports', 'comments'])\n",
    "\n",
    "    # step 3 - remove any nan values\n",
    "    mask = cleaned_df.isnull().any(axis=1)\n",
    "    print(f\"\\n---------\\nRows Removed Due to NaN Labels:\\n{cleaned_df[mask]}\\n---------\\n\")\n",
    "    cleaned_df = cleaned_df.dropna()\n",
    "\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(df, title, filename):\n",
    "    # accuracy\n",
    "    accuracy = accuracy_score(df['nurse_labels'], df['gpt_labels'])\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    # precision\n",
    "    precision = precision_score(df['nurse_labels'], df['gpt_labels'], pos_label='unmet')\n",
    "    print(f\"Precision: {precision}\")\n",
    "    # recall\n",
    "    recall = recall_score(df['nurse_labels'], df['gpt_labels'], pos_label='unmet')\n",
    "    print(f\"Recall: {recall}\")\n",
    "    # f1 score\n",
    "    f1 = f1_score(df['nurse_labels'], df['gpt_labels'], pos_label='unmet')\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    # cohen's kappa\n",
    "    # we use https://www.jstor.org/stable/2529310?seq=4 to determine classifications\n",
    "    cohen_k = cohen_kappa_score(df['nurse_labels'], df['gpt_labels'])\n",
    "    if cohen_k < 0.00:\n",
    "        print(f\"Cohen's Kappa Score: {cohen_k}\\nAgreement Level: Poor\")\n",
    "    elif cohen_k <= 0.2:\n",
    "        print(f\"Cohen's Kappa Score: {cohen_k}\\nAgreement Level: Slight\")\n",
    "    elif cohen_k <= 0.4:\n",
    "        print(f\"Cohen's Kappa Score: {cohen_k}\\nAgreement Level: Fair\")\n",
    "    elif cohen_k <= 0.6:\n",
    "        print(f\"Cohen's Kappa Score: {cohen_k}\\nAgreement Level: Moderate\")\n",
    "    elif cohen_k <= 0.8:\n",
    "        print(f\"Cohen's Kappa Score: {cohen_k}\\nAgreement Level: Substantial\")\n",
    "    elif cohen_k <= 1.0:\n",
    "        print(f\"Cohen's Kappa Score: {cohen_k}\\nAgreement Level: Almost Perfect\")\n",
    "    else:\n",
    "        print(f\"Cohen's Kappa Score: {cohen_k}\\nSomething Went Wrong!\")\n",
    "    \n",
    "    # percentages correct and incorrect\n",
    "    true_all = df[df['nurse_labels'] == df['gpt_labels']].shape[0]\n",
    "    false_all = df[df['nurse_labels'] != df['gpt_labels']].shape[0]\n",
    "    print(f\"\\n---------\\nPercentage of correct notes ~{round((true_all / len(df)) * 100, 2)}%\\nPercentage of incorrect notes ~{round((false_all / len(df)) * 100, 2)}%\\n---------\\n\")\n",
    "\n",
    "\n",
    "    # confusion matrix\n",
    "    conf_matrix = confusion_matrix(df['nurse_labels'], df['gpt_labels'])\n",
    "    ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['met', 'unmet']).plot()\n",
    "    plt.xlabel('GPT Labels')\n",
    "    plt.ylabel('Nurse Labels')\n",
    "    plt.title(title)\n",
    "    plt.savefig(f\"./graphs/{filename}.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nurse_gpt3_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m notes_cleaning(\u001b[43mnurse_gpt3_df\u001b[49m, gpt3_df, remove_comment_notes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m calculate_statistics(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Commented Notes\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNurse Labels vs GPT3 Labels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt3_no_commented_notes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nurse_gpt3_df' is not defined"
     ]
    }
   ],
   "source": [
    "df = notes_cleaning(nurse_gpt3_df, gpt3_df, remove_comment_notes=True)\n",
    "calculate_statistics(df, \"No Commented Notes\\nNurse Labels vs GPT3 Labels\", \"gpt3_no_commented_notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nurse_gpt3_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m notes_cleaning(\u001b[43mnurse_gpt3_df\u001b[49m, gpt3_df, remove_comment_notes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m calculate_statistics(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll Notes\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNurse Labels vs GPT3 Labels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt3_all_notes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nurse_gpt3_df' is not defined"
     ]
    }
   ],
   "source": [
    "df = notes_cleaning(nurse_gpt3_df, gpt3_df, remove_comment_notes=False)\n",
    "calculate_statistics(df, \"All Notes\\nNurse Labels vs GPT3 Labels\", \"gpt3_all_notes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nurse_gpt4_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m notes_cleaning(\u001b[43mnurse_gpt4_df\u001b[49m, gpt4_df, remove_comment_notes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m calculate_statistics(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo Commented Notes\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNurse Labels vs GPT4 Labels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt4_no_commented_notes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nurse_gpt4_df' is not defined"
     ]
    }
   ],
   "source": [
    "df = notes_cleaning(nurse_gpt4_df, gpt4_df, remove_comment_notes=True)\n",
    "calculate_statistics(df, \"No Commented Notes\\nNurse Labels vs GPT4 Labels\", \"gpt4_no_commented_notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nurse_gpt4_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m notes_cleaning(\u001b[43mnurse_gpt4_df\u001b[49m, gpt4_df, remove_comment_notes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m calculate_statistics(df, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll Notes\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mNurse Labels vs GPT4 Labels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt4_all_notes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nurse_gpt4_df' is not defined"
     ]
    }
   ],
   "source": [
    "df = notes_cleaning(nurse_gpt4_df, gpt4_df, remove_comment_notes=False)\n",
    "calculate_statistics(df, \"All Notes\\nNurse Labels vs GPT4 Labels\", \"gpt4_all_notes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
